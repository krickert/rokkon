quarkus:
  application:
    name: embedder-module
  http:
    port: 39100  # Standard internal module port
  generate-code:
    grpc:
      scan-for-proto: com.rokkon.pipeline:protobuf,com.google.api.grpc:proto-google-common-protos
  grpc:
    server:
      use-separate-server: false  # Unified server mode
      host: 0.0.0.0
      enable-reflection-service: true
      max-inbound-message-size: 1073741824  # 1GB
      max-outbound-message-size: 1073741824  # 1GB for responses
    clients:
      embedderService:
        max-inbound-message-size: 1073741824  # 1GB
  log:
    level: INFO
    category:
      "com.rokkon":
        level: DEBUG
  container-image:
    build: false  # Set to true when you want to build the image
    push: false   # Set to true when you want to push to registry
    registry: "registry.rokkon.com:8443"
    image: "pipeline/embedder:${quarkus.application.version}"
    dockerfile-jvm-path: src/main/docker/Dockerfile.jvm
    group: pipeline
    # Credentials should be provided via environment variables:
    # QUARKUS_CONTAINER_IMAGE_USERNAME
    # QUARKUS_CONTAINER_IMAGE_PASSWORD

  # Health and metrics
  smallrye-health:
    root-path: /health
  micrometer:
    enabled: true
    export:
      prometheus:
        enabled: true
        path: /metrics
    binder:
      jvm: true
      system: true
      grpc:
        server: true
        client: true

  # OpenTelemetry configuration
  otel:
    enabled: true
    traces:
      enabled: true
    metrics:
      enabled: true
    logs:
      enabled: true
    exporter:
      otlp:
        # In production, this will be set via environment variable to the sidecar collector
        endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT:http://localhost:4317}
        protocol: ${OTEL_EXPORTER_OTLP_PROTOCOL:grpc}
    resource:
      attributes:
        service.name: embedder-module
        service.namespace: pipeline
        deployment.environment: ${quarkus.profile:prod}

# DJL configuration for ML models
djl:
  engine:
    default-engine: PyTorch
  pytorch:
    # Use GPU if available, otherwise fallback to CPU
    use-gpu: true
    # Number of threads for CPU inference
    num-threads: 4
    # Memory management
    memory-management: ON_SYSTEM_MEMORY_PRESSURE

# Processing buffer configuration
processing:
  buffer:
    enabled: ${PROCESSING_BUFFER_ENABLED:false}
    capacity: ${PROCESSING_BUFFER_CAPACITY:100}
    directory: ${PROCESSING_BUFFER_DIRECTORY:}
    prefix: ${PROCESSING_BUFFER_PREFIX:embedder_output}

# Test profile configuration
"%test":
  quarkus:
    grpc:
      server:
        port: 0  # Use random port for tests
        max-inbound-message-size: 1073741824  # 1GB for tests
      clients:
        embedderService:
          max-inbound-message-size: 1073741824  # 1GB
    # Disable OpenTelemetry in tests
    otel:
      enabled: false
      sdk:
        disabled: true
  processing:
    buffer:
      enabled: true
      directory: ${java.io.tmpdir}/embedder-test-output
  # Disable GPU for tests
  djl:
    pytorch:
      use-gpu: false
